{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste primeiro bloco é realizada a importação das bibliotecas utilizadas no código.\n",
    "- **matplotlib**: Utilizada para impressão dos gráficos dos resultados dos modelos\n",
    "- **pandas**: Utilizada para manipulação do conjunto de dados\n",
    "- **numpy**: Utilizada para operação matemáticas em cima do conjunto de dados\n",
    "- **keras**: Utilizada para criação da rede neural\n",
    "- **time**: Utilizada para marcar o tempo de execução do código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas as configurações referentes a rede neural estão no bloco abaixo. Aqui é possível escolher qual conjunto de dados será utilizado, se o código deverá ou não remover as colunas correlacionadas, como será a divisão de dados entre treino e teste, a função de ativação utilizada e a taxa de aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag para remoção das colunas correlacionadas.\n",
    "CLEAN_DATASET = True\n",
    "\n",
    "# Flag para decisão de qual versão do dataset usar\n",
    "DATASET_COMPLETO = False\n",
    "\n",
    "# Caminho para o dataset\n",
    "# Endereço para download: http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "# Backup: http://web.archive.org/web/*/http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "if DATASET_COMPLETO:\n",
    "    DATASET_PATH = \"D:/UNIFEI/TCC/kdd_dl/dataset/kddcup.data.corrected\"   \n",
    "    # Treino 98%, Validação 1%, Teste 1%\n",
    "    # https://stackoverflow.com/a/13613316\n",
    "    TRAIN_PERCENTAGE = 0.98\n",
    "    VALIDATION_PERCENTAGE = 0.01\n",
    "    TEST_PERCENTAGE = 0.01\n",
    "else:\n",
    "    DATASET_PATH = \"D:/UNIFEI/TCC/kdd_dl/dataset/kddcup.data_10_percent_corrected\"\n",
    "    # Treino 80%, Validação 10%, Teste 10%\n",
    "    TRAIN_PERCENTAGE = 0.8\n",
    "    VALIDATION_PERCENTAGE = 0.1\n",
    "    TEST_PERCENTAGE = 0.1\n",
    "    \n",
    "## Threshold de correlação para exclusão das colunas do dataset\n",
    "CORRCOEF_THRESHOLD = 0.5\n",
    "\n",
    "## Ignora os warnings de divisão por NaN\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "\n",
    "EPOCHS = 100 # Quantas vezes os dados serão corridos completamente\n",
    "BATCH_SIZE = 1024 # Quantas linhas do dataset serão lidas por vez\n",
    "SHUFFLE = True # Mistura os dados antes do treino\n",
    "VERBOSE = 1 # 0 - Desativa os logs durante o treino, 1 - Imprime os logs durante o treino\n",
    "LEARNING_RATE = 0.01 # Taxa de aprendizado\n",
    "ACTIVATION_FN = \"relu\" # Função de ativação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O bloco abaixo realiza a leitura do conjunto de dados e cria um *dataframe* do *pandas* para manipulação mais fácil dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nome das colunas do dataset\n",
    "column_names = [\n",
    "    \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\",\n",
    "    \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\", \"logged_in\",\n",
    "    \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "    \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\",\n",
    "    \"count\", \"srv_count\", \"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "    \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"attack\",\n",
    "]\n",
    "\n",
    "#Leitura do dataset\n",
    "dataset = pd.read_csv(\n",
    "    DATASET_PATH,\n",
    "    header = None,\n",
    "    names = column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o conjunto de dados possui valores léxicos, é necessário trocá-los por valores numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pega os valores  únicos das colunas que tem string\n",
    "protocol_type_values = dataset.protocol_type.unique()\n",
    "service_values = dataset.service.unique()\n",
    "flag_values = dataset.flag.unique()\n",
    "attack_values = dataset.attack.unique()\n",
    "\n",
    "#Transforma as strings em números\n",
    "protocol_type_dict = dict(zip(protocol_type_values, range(len(protocol_type_values))))\n",
    "service_dict = dict(zip(service_values, range(len(service_values))))\n",
    "flag_dict = dict(zip(flag_values, range(len(flag_values))))\n",
    "attack_dict = dict(zip(attack_values, range(len(attack_values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Substitui os valores no dataset que são string por números\n",
    "# dataset = dataset.replace(\n",
    "#     {\"protocol_type\": protocol_type_dict,\n",
    "#      \"service\": service_dict,\n",
    "#      \"flag\": flag_dict,\n",
    "#      \"attack\": attack_dict}\n",
    "# )\n",
    "dataset = dataset.replace({\"attack\": attack_dict})\n",
    "\n",
    "# Pego as colunas numéricas\n",
    "numeric_cols = dataset._get_numeric_data().columns\n",
    "print(numeric_cols)\n",
    "\n",
    "# Pego as colunas categoricas\n",
    "categorical_cols = list(set(dataset.columns)-set(numeric_cols))\n",
    "print(categorical_cols)\n",
    "\n",
    "#Normalizo só as numéricas\n",
    "names_to_normalize = list(dataset.drop(categorical_cols, axis=1).columns)\n",
    "print(names_to_normalize)\n",
    "\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "data_scaled = dataset.copy(deep=True)\n",
    "data_scaled[names_to_normalize] = min_max_scaler.fit_transform(\n",
    "    dataset[names_to_normalize])\n",
    "\n",
    "print(data_scaled.head())\n",
    "\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html\n",
    "def apply_dummies(df, feature):\n",
    "    get_dummies = pd.get_dummies(df[feature])\n",
    "    for x in get_dummies.columns:\n",
    "        dummy_name = f\"{feature}-{x}\"\n",
    "        df[dummy_name] = get_dummies[x]\n",
    "    df.drop(feature, axis=1, inplace=True)\n",
    "    return None\n",
    "\n",
    "for feature in categorical_cols:\n",
    "    if feature not in ['attack', 'target_type']:\n",
    "        apply_dummies(data_scaled, feature)\n",
    "\n",
    "print(\"#\\n#\")\n",
    "print(data_scaled.head())\n",
    "        \n",
    "dataset = data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpeza do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apaga as colunas que tem correlação entre elas que seja maior que CORRCOEF_THRESHOLD\n",
    "\n",
    "#Excluindo só as colunas numéricas\n",
    "\n",
    "if CLEAN_DATASET:\n",
    "    excluded = list()\n",
    "    \n",
    "    for index_1 in range(len(numeric_cols)-1):\n",
    "        for index_2 in range(len(numeric_cols)-1):\n",
    "            if numeric_cols[index_1] not in excluded and numeric_cols[index_1] in numeric_cols and numeric_cols[index_2] not in excluded and index_1 != index_2:\n",
    "                corrcoef = np.corrcoef(dataset[numeric_cols[index_1]], dataset[numeric_cols[index_2]])\n",
    "\n",
    "                if abs(corrcoef[0][1]) > CORRCOEF_THRESHOLD:\n",
    "                    excluded.append(numeric_cols[index_2])\n",
    "                \n",
    "    dataset = dataset[dataset.columns.difference(excluded)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpeza do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separa os dados entre treino e teste\n",
    "train, test = np.split(dataset.sample(frac=1), [int(TRAIN_PERCENTAGE * len(dataset))])\n",
    "\n",
    "train_samples = train.drop(['attack'], axis=1)\n",
    "train_labels = to_categorical(train[['attack']].to_numpy(), num_classes=len(attack_values))\n",
    "\n",
    "test_samples = test.drop(['attack'], axis=1)\n",
    "test_labels = to_categorical(test[['attack']].to_numpy(), num_classes=len(attack_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (train_samples.shape[1],)\n",
    "\n",
    "# Dense(15, activation=ACTIVATION_FN),\n",
    "# Criação do modelo\n",
    "model = Sequential([\n",
    "    Dense(1, input_shape=input_shape, activation=ACTIVATION_FN),\n",
    "    Dense(1, activation=ACTIVATION_FN),\n",
    "    Dense(len(attack_values), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=LEARNING_RATE)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=sgd,\n",
    "    loss='mean_squared_error', \n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# categorical_crossentropy\n",
    "# mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treino da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Treinamento do modelo\n",
    "history = model.fit(\n",
    "    x=train_samples, \n",
    "    y=train_labels, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    epochs=EPOCHS,\n",
    "    shuffle=SHUFFLE, #Mistura os dados\n",
    "    verbose=VERBOSE,\n",
    "    validation_split=VALIDATION_PERCENTAGE #Porcentagem dos dados de treino que serão usadas para validação\n",
    ")\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do modelo\n",
    "h2 = model.evaluate(\n",
    "    x=test_samples,\n",
    "    y=test_labels,\n",
    "    verbose=VERBOSE,\n",
    ")\n",
    "# plot_model(model, to_file='model.png' ,show_shapes=True, show_layer_names=True, expand_nested=True, dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impressão dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file='model.png')\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(h2)\n",
    "\n",
    "print({\n",
    "\"CLEAN_DATASET\":CLEAN_DATASET,\n",
    "\"DATASET_COMPLETO\":DATASET_COMPLETO,\n",
    "\"CORRCOEF_THRESHOLD\":CORRCOEF_THRESHOLD,\n",
    "\"BATCH_SIZE\":BATCH_SIZE,\n",
    "\"EPOCHS\":EPOCHS,\n",
    "\"SHUFFLE\":SHUFFLE,\n",
    "\"VERBOSE\":VERBOSE,\n",
    "\"VALIDATION_SPLIT\":VALIDATION_PERCENTAGE,\n",
    "\"LEARNING_RATE\":LEARNING_RATE,\n",
    "\"ACTIVATION_FN\":ACTIVATION_FN,\n",
    "\"TEMPO\": end_time-start_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "# from keras import backend as K\n",
    "\n",
    "# with a Sequential model\n",
    "# get_3rd_layer_output = K.function([model.layers[0].input],\n",
    "#                                   [model.layers[3].output])\n",
    "# print(get_3rd_layer_output)\n",
    "\n",
    "# print(model.layers[0].input)\n",
    "# print(model.layers[0].output)\n",
    "\n",
    "# print(model.layers[1].input)\n",
    "# print(model.layers[1].output)\n",
    "\n",
    "# print(model.layers[2].input)\n",
    "# print(model.layers[2].output)\n",
    "\n",
    "# print(model.layers[3].input)\n",
    "# print(model.layers[3].output)\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "# input1 = model.input # input placeholder\n",
    "\n",
    "# output1 = [layer.output for layer in model.layers]# all layer outputs\n",
    "\n",
    "# fun = K.function([input1, K.learning_phase()],output1)# evaluation function\n",
    "\n",
    "# # Testing\n",
    "\n",
    "# t = np.random.random((41,))[np.newaxis,...]\n",
    "\n",
    "# t = np.array([[0,1,19,9,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00]])\n",
    "\n",
    "# layer_outputs = fun([t, 1])\n",
    "\n",
    "\n",
    "\n",
    "# print(layer_outputs[0]) #printing the outputs of layers\n",
    "# print(layer_outputs[1]) #printing the outputs of layers\n",
    "# print(layer_outputs[2]) #printing the outputs of layers\n",
    "# print(layer_outputs[3]) #printing the outputs of layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
